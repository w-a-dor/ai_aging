{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-07-30T13:17:52.247690Z",
     "start_time": "2025-07-30T13:17:48.546601Z"
    }
   },
   "source": [
    "# --- Imports ---\n",
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/weronikadorociak/Documents/LSE/MY474 Applied Machine Learning/Machine Learning Python/venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# --- Load data ---\n",
    "eurostat = pd.read_csv('data/eurostat.csv')\n",
    "gpts_df = pd.read_csv('data/gpts_are_gpts.csv')\n",
    "nace = pd.read_csv('data/economic_activity_sector.csv')"
   ],
   "id": "2bd22afab496abbb"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-30T13:18:26.547499Z",
     "start_time": "2025-07-30T13:18:26.526772Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# --- Prepare economic activity categories ---\n",
    "economic_activities = eurostat[\"nace_r2\"].unique()\n",
    "\n",
    "# --- Define sentence embedding models to compare ---\n",
    "models = [\n",
    "    \"all-MiniLM-L6-v2\",\n",
    "    \"all-mpnet-base-v2\",\n",
    "    \"paraphrase-multilingual-MiniLM-L12-v2\",\n",
    "    \"sentence-transformers/bert-base-nli-mean-tokens\",\n",
    "]"
   ],
   "id": "5051c371fdd1c86",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'eurostat' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[3], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# --- Prepare economic activity categories ---\u001B[39;00m\n\u001B[0;32m----> 2\u001B[0m economic_activities \u001B[38;5;241m=\u001B[39m \u001B[43meurostat\u001B[49m[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnace_r2\u001B[39m\u001B[38;5;124m\"\u001B[39m]\u001B[38;5;241m.\u001B[39munique()\n\u001B[1;32m      4\u001B[0m \u001B[38;5;66;03m# --- Define sentence embedding models to compare ---\u001B[39;00m\n\u001B[1;32m      5\u001B[0m models \u001B[38;5;241m=\u001B[39m [\n\u001B[1;32m      6\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mall-MiniLM-L6-v2\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m      7\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mall-mpnet-base-v2\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m      8\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mparaphrase-multilingual-MiniLM-L12-v2\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m      9\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msentence-transformers/bert-base-nli-mean-tokens\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m     10\u001B[0m ]\n",
      "\u001B[0;31mNameError\u001B[0m: name 'eurostat' is not defined"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-30T13:18:21.899089Z",
     "start_time": "2025-07-30T13:18:21.797961Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# --- Perform semantic matching for each model ---\n",
    "records = []\n",
    "for model_name in models:\n",
    "    print(f\"Processing {model_name}\")\n",
    "    model = SentenceTransformer(model_name)\n",
    "\n",
    "    task_emb = model.encode(gpts_df['Title'].tolist(), convert_to_tensor=True)\n",
    "    econ_emb = model.encode(economic_activities.tolist(), convert_to_tensor=True)\n",
    "\n",
    "    sims = cosine_similarity(task_emb.cpu().numpy(), econ_emb.cpu().numpy())\n",
    "    best_idx = sims.argmax(axis=1)\n",
    "\n",
    "    for i, task in enumerate(gpts_df['Title']):\n",
    "        rec = {\n",
    "            'Title': task,\n",
    "            model_name: economic_activities[best_idx[i]],\n",
    "            'human_beta': gpts_df.loc[i, 'human_beta']\n",
    "        }\n",
    "        records.append(rec)"
   ],
   "id": "af0823d032fdf176",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'models' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[2], line 3\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# --- Perform semantic matching for each model ---\u001B[39;00m\n\u001B[1;32m      2\u001B[0m records \u001B[38;5;241m=\u001B[39m []\n\u001B[0;32m----> 3\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m model_name \u001B[38;5;129;01min\u001B[39;00m \u001B[43mmodels\u001B[49m:\n\u001B[1;32m      4\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mProcessing \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mmodel_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m      5\u001B[0m     model \u001B[38;5;241m=\u001B[39m SentenceTransformer(model_name)\n",
      "\u001B[0;31mNameError\u001B[0m: name 'models' is not defined"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-30T13:18:43.152451Z",
     "start_time": "2025-07-30T13:18:43.118693Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# --- Extract and merge predictions from all models ---\n",
    "all_MiniLM_L6_v2 = df_long[['Title', 'human_beta', 'all-MiniLM-L6-v2']].dropna()\n",
    "all_mpnet_base_v2 = df_long[['Title', 'human_beta', 'all-mpnet-base-v2']].dropna()\n",
    "MiniLM_L12_v2 = df_long[['Title', 'human_beta', 'paraphrase-multilingual-MiniLM-L12-v2']].dropna()\n",
    "bert = df_long[['Title', 'human_beta', 'sentence-transformers/bert-base-nli-mean-tokens']].dropna()\n",
    "\n",
    "df_full = (\n",
    "    all_MiniLM_L6_v2\n",
    "    .merge(all_mpnet_base_v2, on=['Title', 'human_beta'])\n",
    "    .merge(MiniLM_L12_v2, on=['Title', 'human_beta'])\n",
    "    .merge(bert, on=['Title', 'human_beta'])\n",
    ")"
   ],
   "id": "f54048903f87f95f",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_long' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[4], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# --- Extract and merge predictions from all models ---\u001B[39;00m\n\u001B[0;32m----> 2\u001B[0m all_MiniLM_L6_v2 \u001B[38;5;241m=\u001B[39m \u001B[43mdf_long\u001B[49m[[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mTitle\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mhuman_beta\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mall-MiniLM-L6-v2\u001B[39m\u001B[38;5;124m'\u001B[39m]]\u001B[38;5;241m.\u001B[39mdropna()\n\u001B[1;32m      3\u001B[0m all_mpnet_base_v2 \u001B[38;5;241m=\u001B[39m df_long[[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mTitle\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mhuman_beta\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mall-mpnet-base-v2\u001B[39m\u001B[38;5;124m'\u001B[39m]]\u001B[38;5;241m.\u001B[39mdropna()\n\u001B[1;32m      4\u001B[0m MiniLM_L12_v2 \u001B[38;5;241m=\u001B[39m df_long[[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mTitle\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mhuman_beta\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mparaphrase-multilingual-MiniLM-L12-v2\u001B[39m\u001B[38;5;124m'\u001B[39m]]\u001B[38;5;241m.\u001B[39mdropna()\n",
      "\u001B[0;31mNameError\u001B[0m: name 'df_long' is not defined"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# --- Count model agreement per task ---\n",
    "model_cols = [\n",
    "    'all-MiniLM-L6-v2',\n",
    "    'all-mpnet-base-v2',\n",
    "    'paraphrase-multilingual-MiniLM-L12-v2',\n",
    "    'sentence-transformers/bert-base-nli-mean-tokens'\n",
    "]\n",
    "\n",
    "\n",
    "def count_model_agreement(row):\n",
    "    return row[model_cols].value_counts().max()\n",
    "\n",
    "\n",
    "df_full['num_models_agree'] = df_full.apply(count_model_agreement, axis=1)"
   ],
   "id": "cb155fade86f13ff"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# --- Load ChatGPT fallback predictions for disagreement cases ---\n",
    "chatgpt = pd.read_excel(\"data/disagremment_chatgpt_gpts.xlsx\")\n",
    "\n",
    "# --- Merge model predictions with ChatGPT 4o output ---\n",
    "df = df_full.merge(chatgpt, on='Title', how='outer')\n",
    "\n",
    "\n",
    "# --- Decide final label for each task ---\n",
    "def choose_final_prediction(row):\n",
    "    if row['num_models_agree'] == 1:\n",
    "        return row['chat_gpt_4o']\n",
    "    else:\n",
    "        votes = row[model_cols].dropna().value_counts()\n",
    "        if not votes.empty:\n",
    "            return votes.idxmax()\n",
    "        else:\n",
    "            return row['chat_gpt_4o']  # Fallback if all are NaN\n",
    "\n",
    "\n",
    "df['economic_activity_eurostat'] = df.apply(choose_final_prediction, axis=1)\n",
    "\n",
    "# --- Filter valid records with non-null human_beta ---\n",
    "df = df.dropna(subset=['human_beta'])"
   ],
   "id": "fb41a05fde40cd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# --- Merge with NACE sector info and calculate mean AI potential ---\n",
    "ai_potential = df.merge(nace, how='left', left_on='economic_activity_eurostat', right_on='Economic Activity')\n",
    "ai_potential = ai_potential.groupby(\"sector\")[\"human_beta\"].mean().reset_index()\n",
    "ai_potential[\"human_beta\"] = ai_potential[\"human_beta\"] * 100  # Convert to percentage\n",
    "ai_potential.columns = [\"sector\", \"ai_potential\"]\n",
    "\n",
    "# --- Export result to CSV ---\n",
    "ai_potential.to_csv(\"data/ai_potential_sector.csv\", index=False)"
   ],
   "id": "97c1a92f02f6baaf"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
